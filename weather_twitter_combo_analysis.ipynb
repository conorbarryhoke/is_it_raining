{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "import twitter, re, datetime, pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import tweepy\n",
    "from pprint import pprint \n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kelvin_converter(deg_k):\n",
    "    \n",
    "    return (deg_k - 273.15) * 9/5 + 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_path = \"http://storage.googleapis.com/twitter-weather/weather-data/\"\n",
    "\n",
    "with open('./info/weather_key.txt', \"r\") as file:\n",
    "    api_key_weather = file.read().replace('\\n', '')\n",
    "    \n",
    "# https://www.geeksforgeeks.org/python-find-current-weather-of-any-city-using-openweathermap-api/\n",
    "\n",
    "base_url = \"http://api.openweathermap.org/data/2.5/weather?\"\n",
    "city_name = \"Austin\"\n",
    "complete_url = base_url + \"appid=\" + api_key_weather + \"&q=\" + city_name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(complete_url) \n",
    "x = response.json() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare vars\n",
    "weather_main = None\n",
    "weather_description = None\n",
    "temp_K = None\n",
    "temp_F = None\n",
    "pressure = None\n",
    "humidity = None\n",
    "temp_min = None\n",
    "temp_max = None\n",
    "sea_level = None\n",
    "grnd_level = None\n",
    "visibility = None\n",
    "wind_speed = None\n",
    "wind_deg = None\n",
    "rain_type = None\n",
    "rain_list = None\n",
    "clouds = None\n",
    "city_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad var sea_level\n",
      "bad var grnd_level\n",
      "bad var rain_type\n",
      "bad var rain_list\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    city_name = x['name']\n",
    "except:\n",
    "    print('bad var city')\n",
    "try:\n",
    "    weather_main = x['weather'][0]['main']\n",
    "except:\n",
    "    print('bad var weather_main')\n",
    "    \n",
    "try:\n",
    "    weather_description = x['weather'][0]['description']\n",
    "except:\n",
    "    print('bad var weather_description')\n",
    "    \n",
    "try:\n",
    "    temp_K = x['main']['temp']\n",
    "    temp_F = kelvin_converter(temp_K)\n",
    "except:\n",
    "    print('bad var temp_K')\n",
    "    \n",
    "try:\n",
    "    pressure = x['main']['pressure']\n",
    "except:\n",
    "    print('bad var pressure')\n",
    "        \n",
    "try:\n",
    "    humidity = x['main']['humidity']\n",
    "except:\n",
    "    print('bad var humidity')\n",
    "        \n",
    "try:\n",
    "    temp_min = x['main']['temp_min']\n",
    "except:\n",
    "    print('bad var temp_min')\n",
    "        \n",
    "try:\n",
    "    temp_max = x['main']['temp_max']\n",
    "except:\n",
    "    print('bad var temp_max')\n",
    "        \n",
    "try:\n",
    "    sea_level = x['main']['sea_level']\n",
    "except:\n",
    "    print('bad var sea_level')\n",
    "        \n",
    "try:\n",
    "    grnd_level = x['main']['grnd_level']\n",
    "except:\n",
    "    print('bad var grnd_level')\n",
    "        \n",
    "try:\n",
    "    visibility = x['visibility']\n",
    "except:\n",
    "    print('bad var visibility')\n",
    "        \n",
    "try:\n",
    "    wind_speed = x['wind']['speed']\n",
    "except:\n",
    "    print('bad var wind_speed')\n",
    "        \n",
    "try:\n",
    "    wind_deg = x['wind']['deg']\n",
    "except:\n",
    "    print('bad var wind_deg')\n",
    "        \n",
    "try:\n",
    "    rain_type = list(x['rain'].keys())[0]\n",
    "except:\n",
    "    print('bad var rain_type')\n",
    "        \n",
    "try:\n",
    "    rain_list = x['rain'][rain_type]\n",
    "except:\n",
    "    print('bad var rain_list')\n",
    "        \n",
    "try:\n",
    "    clouds = x['clouds']['all']\n",
    "except:\n",
    "    print('bad var clouds')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'timestamp': timestamp, \n",
    "    'weather_main' :weather_main, \n",
    "    'weather_description'  :weather_description ,\n",
    "    'temp_K'  :temp_K ,\n",
    "    'temp_F'  :temp_F ,\n",
    "    'pressure'  :pressure ,\n",
    "    'humidity'  : humidity,\n",
    "    'temp_min'  :temp_min ,\n",
    "    'temp_max'  :temp_max ,\n",
    "    'sea_level' : sea_level,\n",
    "    'grnd_level' : grnd_level,\n",
    "    'visibility' : visibility,\n",
    "    'wind_speed' :wind_speed ,\n",
    "    'wind_deg' : wind_deg,\n",
    "    'rain_type': rain_type,  \n",
    "    'rain_list': rain_list, \n",
    "    'clouds':clouds , \n",
    "    'city_name':city_name, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(data_dict, index=[0]).to_csv('./info/weather_start.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "            './info/log_queries.json',\n",
    "            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_base = \"\"\"\n",
    "INSERT INTO metridaticsmain.webscrapes.weather_data (\n",
    "timestamp,\n",
    "weather_main,\n",
    "weather_description,\n",
    "temp_K,\n",
    "temp_F,\n",
    "pressure,\n",
    "humidity,\n",
    "temp_min,\n",
    "temp_max,\n",
    "sea_level,\n",
    "grnd_level,\n",
    "visibility,\n",
    "wind_speed,\n",
    "wind_deg,\n",
    "rain_type,\n",
    "rain_list,\n",
    "clouds,\n",
    "city_name\n",
    ")\n",
    " \n",
    "VALUES ('{}', '{}', '{}', {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, '{}', {}, {}, '{}')\n",
    "\n",
    "\n",
    ";\n",
    "\"\"\".format(timestamp,\n",
    "weather_main,\n",
    "weather_description,\n",
    "temp_K,\n",
    "temp_F,\n",
    "pressure,\n",
    "humidity,\n",
    "temp_min,\n",
    "temp_max,\n",
    "sea_level,\n",
    "grnd_level,\n",
    "visibility,\n",
    "wind_speed,\n",
    "wind_deg,\n",
    "rain_type,\n",
    "rain_list,\n",
    "clouds,\n",
    "city_name).replace('\\n', '').replace(\"None\",\"NULL\").replace(\"'NULL'\", \"NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7f4c351f3c88>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_job = client.query(q_base, location=\"US\")\n",
    "query_job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get New Data (Script Needs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One time\n",
    "with open('./info/twitterapi2.txt') as f:\n",
    "    ck, cs, atk, ats = f.read().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your keys go here:\n",
    "twitter_keys = {\n",
    "    'consumer_key':        ck.replace('\\n', ''),\n",
    "    'consumer_secret':     cs.replace('\\n', ''),\n",
    "    'access_token_key':    atk.replace('\\n', ''),\n",
    "    'access_token_secret': ats.replace('\\n', '')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(twitter_keys['consumer_key'], twitter_keys['consumer_secret'])\n",
    "auth.set_access_token(twitter_keys['access_token_key'], twitter_keys['access_token_secret'])\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old version\n",
    "# update_interval_hours = 6\n",
    "# for i in range(24):\n",
    "#     sleep(update_interval_hours*60*60)\n",
    "\n",
    "# # Each Time\n",
    "# end_date = '{}-{}-{}'.format(tomorrow.year, tomorrow.month, tomorrow.day)   \n",
    "# start_date = '{}-{}-{}'.format(yesterday.year, yesterday.month, yesterday.day)\n",
    "\n",
    "# Otherwise, won't update public doc for 1 hour\n",
    "# !gsutil -h \"Cache-Control:no-cache, max-age=0\" cp -a public-read myfile.json gs://mybucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = datetime.datetime.now() - datetime.timedelta(days=1)\n",
    "time_stop = datetime.datetime.now() + datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each Time\n",
    "start_date = '{}-{}-{}'.format(time_start.year, time_start.month, time_start.day)\n",
    "end_date = '{}-{}-{}'.format(time_stop.year, time_stop.month, time_stop.day)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more tweets found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tweepy.binder:Rate limit reached. Sleeping for: 499\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-34cfdc6f2a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mplace_q\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeo_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplace_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"city\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mplace_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m                                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_on_rate_limit_notify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                                         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rate limit reached. Sleeping for: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msleep_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# sleep for few extra sec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;31m# if self.wait_on_rate_limit and self._reset_time is not None and \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "create_big = True\n",
    "if create_big:\n",
    "    df_big = pd.DataFrame()\n",
    "    places = [\"Austin, TX\", \"Portland, OR\"]\n",
    "\n",
    "else:\n",
    "    df_small = pd.DataFrame()\n",
    "#     places = [\"Austin, TX\"]\n",
    "    places = [\"Austin, TX\"]\n",
    "tweetsPerQry = 100\n",
    "counter = 0\n",
    "\n",
    "for place_q in places: \n",
    "\n",
    "    place = api.geo_search(query=place_q, granularity=\"city\")\n",
    "    place_id = place[0].id\n",
    "\n",
    "    max_id = -1\n",
    "    for _ in range(20):\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                new_tweets = api.search(q='place:%s' % place_id, count=tweetsPerQry, since=start_date, until=end_date)\n",
    "            else:\n",
    "                new_tweets = api.search(q='place:%s' % place_id, count=tweetsPerQry, max_id=str(max_id - 1), since=start_date, until=end_date)\n",
    "\n",
    "            df_text = pd.DataFrame([new_tweets[i]._json['text'] for i in range(len(new_tweets))], columns=['tweet_text'])\n",
    "            df_text['tweet_time']=[new_tweets[i]._json['created_at'] for i in range(len(new_tweets))]\n",
    "            df_text['tweet_place'] = place_q\n",
    "            if create_big:\n",
    "                df_big = pd.concat([df_big, df_text])\n",
    "            else:\n",
    "                df_small = pd.concat([df_small, df_text])\n",
    "            \n",
    "            if not new_tweets:\n",
    "                counter += 1\n",
    "#                 if create_big:\n",
    "#                     df_big.to_csv('./collected_tweets_{}_{}.csv'.format(start_date, counter)) \n",
    "#                 else: \n",
    "#                     df_small.to_csv('./collected_tweets_{}_{}.csv'.format(start_date, counter)) # Should provide log of last good pull at least\n",
    "                    \n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError as e:\n",
    "            print('    all_done')\n",
    "            break\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7f4c34550fd0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q_base = \"\"\"\n",
    "# CREATE TABLE metridaticsmain.webscrapes.twitter_data_raw (\n",
    "#  tweet_time timestamp,\n",
    "#  tweet_text STRING,\n",
    "#  tweet_place STRING\n",
    "\n",
    "# );\n",
    "# \"\"\"\n",
    "\n",
    "# query_job = client.query(q_base, location=\"US\")\n",
    "# query_job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big['tweet_time'] = pd.to_datetime(df_big.tweet_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_chars(val_string):\n",
    "    val_string = val_string.replace(\"'\", \"\")\n",
    "    val_string = val_string.replace(\"\\\\\", \"\")\n",
    "    \n",
    "    return val_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tweet_info(tweet_text, tweet_time, tweet_place):\n",
    "    tweet_text = clean_chars(tweet_text)\n",
    "    \n",
    "    return \"('\" + tweet_place + \"', '\" + str(tweet_time) + \"', '\" + tweet_text + \"') \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_combined = df_big[df_big.tweet_place=='Austin, TX'].iloc[:500,:].apply(lambda x: combine_tweet_info(x['tweet_text'], x['tweet_time'], x['tweet_place'] ), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_combined = ', '.join(values_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_base = \"\"\"\n",
    "INSERT INTO metridaticsmain.webscrapes.twitter_data_raw (\n",
    " tweet_place,\n",
    " tweet_time,\n",
    " tweet_text\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    " VALUES {}\n",
    ";\n",
    "\"\"\".format(values_combined)\n",
    "q_base = q_base.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7f4c33ad75f8>"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_job = client.query(q_base, location=\"US\")\n",
    "query_job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"('Austin, TX', '2019-09-27 03:06:09+00:00', 'Omg Jordan Howard have a game! #PHIvGB') , ('Austin, TX\""
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values_combined[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "sys.path.append(cwd+'/scripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import import_module\n",
    "import_module(\"my_bigq\")\n",
    "import_module(\"run_weather\")\n",
    "from my_bigq import bigquery_handler \n",
    "import run_weather as rw\n",
    "import run_twitter as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad var sea_level\n",
      "bad var grnd_level\n",
      "bad var rain_type\n",
      "bad var rain_list\n"
     ]
    }
   ],
   "source": [
    "_test = rw.run_city(\"Austin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rw.get_weather(\"Austin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad var sea_level\n",
      "bad var grnd_level\n",
      "bad var rain_type\n",
      "bad var rain_list\n"
     ]
    }
   ],
   "source": [
    "data_dict = rw.process_weather(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test2 = bigquery_handler(q_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test3 = _test2.run_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7f6880704e80>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = rt.define_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big = rt.get_twitter(api, \"Austin, TX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_big.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'run_twitter' from '/home/conor/projects/is_it_raining/scripts/run_twitter.py'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1%24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1 chunks\n"
     ]
    }
   ],
   "source": [
    "values_combined = rt.process_twitter(df_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_chars(val_string):\n",
    "    val_string = val_string.replace(\"'\", \"\")\n",
    "    val_string = val_string.replace(\"\\\\\", \"\")\n",
    "\n",
    "    return val_string\n",
    "\n",
    "def combine_tweet_info(tweet_text, tweet_time, tweet_place):\n",
    "    tweet_text = clean_chars(tweet_text)\n",
    "\n",
    "    return \"('\" + tweet_place + \"', '\" + str(tweet_time) + \"', '\" + tweet_text + \"') \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1 chunks\n"
     ]
    }
   ],
   "source": [
    "num_rows = df_big.shape[0]\n",
    "values_combined = []\n",
    "n_chunks = int(floor(num_rows/500))\n",
    "print('found {} chunks'.format(n_chunks))\n",
    "\n",
    "for n in range(n_chunks+1):\n",
    "    _rel = df_big.iloc[n*500: (n+1)*500, :]\n",
    "    if _rel.shape[0] == 0: # cases where df_big is a multiple of 1000\n",
    "        continue\n",
    "    _rel_combined = _rel.apply(lambda x: combine_tweet_info(x['tweet_text'], x['tweet_time'], x['tweet_place'] ), axis=1).values\n",
    "    values_combined.append(_rel_combined)\n",
    "\n",
    "for i in range(len(values_combined)):\n",
    "    values_combined[i] = ', '.join(values_combined[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<google.cloud.bigquery.table._EmptyRowIterator at 0x7f6842af5a20>,\n",
       " <google.cloud.bigquery.table._EmptyRowIterator at 0x7f6842ea5e10>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.send_to_table(values_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_big.to_csv('./5_cities_2019.1.17.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = cvec.get_feature_names()\n",
    "df_small = pd.concat([df_small.reset_index().drop('index', axis=1), pd.DataFrame(cvec.transform(df_small.tweet_text.str.lower()).todense(), columns=cvec.get_feature_names())], axis=1)\n",
    "_X = df_small.loc[:, [col for col in df_small.columns if col in model_columns]]\n",
    "\n",
    "_X.fillna(0, inplace=True)\n",
    "\n",
    "df_small['predicted'] = lr.predict(_X)\n",
    "df_small['probas'] = [element[1] for element in lr.predict_proba(_X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "excludes = ['tweet_text', 'tweet_time', 'tweet_place', 'is_rain', 'predicted']\n",
    "_rain_probability = df_small.loc[:, excludes + ['predicted', 'probas']].groupby('tweet_place').mean()['probas'].values[0]\n",
    "_rain_probability = round(_rain_probability*100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refit model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = pickle.load(open('vectorizer.sav', 'rb')) \n",
    "lr = pickle.load(open('lin_regressor.sav', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big=pd.concat([\n",
    "    pd.read_csv('./data/5_cities_2019.1.3.csv'), \n",
    "    pd.read_csv('./data/5_cities_2019.1.17.csv'), \n",
    "#     pd.read_csv('./data/austin_2019_02_19.csv')\n",
    "])\n",
    "df_big.index = range(df_big.shape[0])\n",
    "df_big[\"tweet_time\"] = pd.to_datetime(df_big.tweet_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_place</th>\n",
       "      <th>tweet_time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">Austin, TX</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">Boston, MA</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">Portland, OR</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">San Francisco, CA</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">Shreveport, LA</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              tweet_text\n",
       "tweet_place       tweet_time            \n",
       "Austin, TX        2019-01-03         358\n",
       "                  2019-01-17         788\n",
       "Boston, MA        2019-01-03          82\n",
       "                  2019-01-17         625\n",
       "Portland, OR      2019-01-03         442\n",
       "                  2019-01-17         654\n",
       "San Francisco, CA 2019-01-03         316\n",
       "                  2019-01-17         654\n",
       "Shreveport, LA    2019-01-03         455\n",
       "                  2019-01-17         474"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_big.groupby(['tweet_place', df_big.tweet_time.dt.date]).count().loc[:, [\"tweet_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "\n",
    "df_big=df_big.join(pd.DataFrame(cvec.fit_transform(df_big.tweet_text.str.lower()).todense(), columns=cvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big[\"tweet_time\"] = pd.to_datetime(df_big.tweet_time)\n",
    "df_big['is_rain']= (((df_big.tweet_place == \"Shreveport, LA\") & (df_big.tweet_time.dt.date == datetime.date(2019, 1, 3))) |\\\n",
    "                    ((df_big.tweet_place == \"San Francisco, CA\") & (df_big.tweet_time.dt.date == datetime.date(2019, 1, 17)))|\n",
    "                    ((df_big.tweet_place == \"Austin, TX\") & (df_big.tweet_time.dt.date == datetime.date(2019, 2, 20)))\n",
    "                   )*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "excludes = ['tweet_text', 'tweet_time', 'tweet_place', 'is_rain', 'predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_big.loc[:, [col for col in df_big.columns if col not in excludes]]\n",
    "y = df_big.is_rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6125721616420783\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_rain</th>\n",
       "      <th>predicted</th>\n",
       "      <th>predicted</th>\n",
       "      <th>probas</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_place</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austin, TX</th>\n",
       "      <td>0.547393</td>\n",
       "      <td>0.467615</td>\n",
       "      <td>0.467615</td>\n",
       "      <td>0.463345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Boston, MA</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052334</td>\n",
       "      <td>0.052334</td>\n",
       "      <td>0.224023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portland, OR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043796</td>\n",
       "      <td>0.043796</td>\n",
       "      <td>0.200735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>San Francisco, CA</th>\n",
       "      <td>0.674227</td>\n",
       "      <td>0.555670</td>\n",
       "      <td>0.555670</td>\n",
       "      <td>0.516902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shreveport, LA</th>\n",
       "      <td>0.489774</td>\n",
       "      <td>0.392896</td>\n",
       "      <td>0.392896</td>\n",
       "      <td>0.432043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    is_rain  predicted  predicted    probas\n",
       "tweet_place                                                \n",
       "Austin, TX         0.547393   0.467615   0.467615  0.463345\n",
       "Boston, MA         0.000000   0.052334   0.052334  0.224023\n",
       "Portland, OR       0.000000   0.043796   0.043796  0.200735\n",
       "San Francisco, CA  0.674227   0.555670   0.555670  0.516902\n",
       "Shreveport, LA     0.489774   0.392896   0.392896  0.432043"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_test, y_test))\n",
    "\n",
    "df_big['predicted'] = lr.predict(X)\n",
    "df_big['probas'] = [element[1] for element in lr.predict_proba(X)]\n",
    "\n",
    "df_big.loc[:, excludes + ['predicted', 'probas']].groupby('tweet_place').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.648584596895982"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_big[(df_big.tweet_place==\"Austin, TX\")&(df_big.tweet_time.dt.date>datetime.date(2019,2,1))].loc[:, 'probas'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "probas    0.646408\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doesn't seem to help too much filter \n",
    "df_big[(df_big.tweet_place==\"Austin, TX\")&(df_big.tweet_time.dt.date>=datetime.date(2019,2,17))]\\\n",
    ".loc[:, ['tweet_time','probas']].sort_values(by='tweet_time').tail(20).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(cvec, open('vectorizer.sav', 'wb')) \n",
    "# pickle.dump(lr, open('lin_regressor.sav', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = pickle.load(open('vectorizer.sav', 'rb')) \n",
    "lr = pickle.load(open('lin_regressor.sav', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check small frame\n",
    "df_small = pd.read_csv('./data/austin_2019-01-27_to_2019-01-28.csv')\n",
    "df_small[\"tweet_time\"] = pd.to_datetime(df_small.tweet_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 to 2 hours ago had 363 tweets\n",
      "2 to 3 hours ago had 291 tweets\n",
      "3 to 4 hours ago had 190 tweets\n",
      "4 to 5 hours ago had 120 tweets\n",
      "5 to 6 hours ago had 57 tweets\n",
      "6 to 7 hours ago had 53 tweets\n",
      "7 to 8 hours ago had 77 tweets\n",
      "8 to 9 hours ago had 81 tweets\n",
      "9 to 10 hours ago had 67 tweets\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    print('{} to {} hours ago had {} tweets'.format(i , i+1, \n",
    "                                                   df_small[(df_small.tweet_time >= (datetime.datetime.now() - datetime.timedelta(hours=i+1)))&\\\n",
    "                                                          ( df_small.tweet_time <= (datetime.datetime.now() - datetime.timedelta(hours=i)))].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_small = df_small[df_small.tweet_time > (datetime.datetime.now() - datetime.timedelta(hours=4))] # Expand here to 6 if shape[0], then allow all\n",
    "df_small = pd.concat([df_small, pd.DataFrame(cvec.transform(df_small.tweet_text.str.lower()).todense(), columns=cvec.get_feature_names())], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = df_small.loc[:, [col for col in df_small.columns if col in X.columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_rain</th>\n",
       "      <th>predicted</th>\n",
       "      <th>predicted</th>\n",
       "      <th>probas</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_place</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austin, TX</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254275</td>\n",
       "      <td>0.254275</td>\n",
       "      <td>0.395661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             is_rain  predicted  predicted    probas\n",
       "tweet_place                                         \n",
       "Austin, TX       NaN   0.254275   0.254275  0.395661"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_X.fillna(0, inplace=True)\n",
    "\n",
    "df_small['predicted'] = lr.predict(_X)\n",
    "df_small['probas'] = [element[1] for element in lr.predict_proba(_X)]\n",
    "\n",
    "df_small.loc[:, excludes + ['predicted', 'probas']].groupby('tweet_place').mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

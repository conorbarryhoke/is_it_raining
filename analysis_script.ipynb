{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, pandas as pd\n",
    "import os, sys\n",
    "import pytz\n",
    "import pickle \n",
    "from importlib import import_module\n",
    "\n",
    "# Custom Imports and scripts\n",
    "cwd = os.getcwd()\n",
    "sys.path.append(cwd+'/scripts/')\n",
    "import_module(\"my_bigq\")\n",
    "\n",
    "from my_bigq import bigquery_handler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize\n",
    "def city_mapper(x):\n",
    "    if x == 'Austin, TX':\n",
    "        return 'Austin'\n",
    "    if x == 'Seattle, WA':\n",
    "        return 'Seattle'\n",
    "    return 'other'\n",
    "\n",
    "# Vectorizer\n",
    "simple_cvec = pickle.load(open('cvec_simple.sav', 'rb'))\n",
    "\n",
    "with open(\"cvec_feats.txt\", \"r\") as f:\n",
    "    good_feats = f.read()\n",
    "    f.close()\n",
    "good_feats = good_feats.split(', ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = pickle.load(open('new_log_reg_v2.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_rain(probability):\n",
    "    if probability > .8:\n",
    "        return 'Its totally raining!'\n",
    "    if probability > .5:\n",
    "        return 'So Probably'\n",
    "    if probability > .2:\n",
    "        return 'Probably not raining'\n",
    "    return 'Definitely not raining!'\n",
    "\n",
    "\n",
    "f_path_full = \"gs://twitter-weather/html-statements-public/isitraining.html\"\n",
    "f_path_date = \"gs://twitter-weather/html-statements-public/update_date.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "q_base_twitter = \"\"\"\n",
    "SELECT * \n",
    "FROM metridaticsmain.webscrapes.twitter_data_raw\n",
    "WHERE tweet_place = 'Austin, TX'\n",
    "\"\"\"\n",
    "\n",
    "# https://googleapis.dev/python/bigquery/latest/usage/pandas.html\n",
    "df_twitter_r = bigquery_handler(q_base = q_base_twitter).run_query(how='selects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-dupe and downselect cities\n",
    "df_twitter_r = df_twitter_r.drop_duplicates(['tweet_time', 'tweet_text', 'tweet_place']).reset_index().drop('index', axis=1)\n",
    "df_twitter_r = df_twitter_r[df_twitter_r.tweet_place.isin(['Austin, TX', 'Seattle, WA'])].reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets Found:  13588\n"
     ]
    }
   ],
   "source": [
    "df_twitter_r['hour_trunc'] = df_twitter_r.tweet_time.dt.floor('h')\n",
    "df_twitter_r['city_name'] = df_twitter_r['tweet_place'].apply(lambda x: city_mapper(x))\n",
    "\n",
    "print('Tweets Found: ', df_twitter_r.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downselect to prevent too much vectorization\n",
    "tv_end = datetime.datetime.timestamp(df_twitter_r.tweet_time.max())\n",
    "tv_end = datetime.datetime.fromtimestamp(tv_end)\n",
    "\n",
    "tv_start = tv_end- datetime.timedelta(days=2)\n",
    "tv_start = tv_start.replace(tzinfo=pytz.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_r = df_twitter_r[df_twitter_r.tweet_time>tv_start].reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conor/.local/lib/python3.6/site-packages/pandas/core/indexing.py:1418: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "# load and transform twitter\n",
    "twitter_vectorized = simple_cvec.fit_transform(df_twitter_r.tweet_text)\n",
    "twitter_vectorized = pd.DataFrame(twitter_vectorized.toarray(), columns=simple_cvec.get_feature_names())\n",
    "twitter_vectorized = twitter_vectorized.loc[:, good_feats]\n",
    "twitter_vectorized['tweet_time'] = df_twitter_r['tweet_time']\n",
    "twitter_vectorized['is_Austin'] = (df_twitter_r['city_name'] == 'Austin')\n",
    "twitter_vectorized.set_index('tweet_time', inplace=True)\n",
    "\n",
    "twitter_vectorized = twitter_vectorized.loc[:, good_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rolling\n",
    "tv_austin = twitter_vectorized\n",
    "\n",
    "# simple: From 25 until 2 hours ago\n",
    "tv_austin_1s = tv_austin.resample(\"1T\").sum().fillna(0)\n",
    "tv_austin_25h = tv_austin_1s.rolling(window=25*60, min_periods=1).sum()\n",
    "tv_austin_2h = tv_austin_1s.rolling(window=2*60, min_periods=1).sum()\n",
    "\n",
    "tv_austin_diff = tv_austin_25h - tv_austin_2h\n",
    "tv_austin_diff['hour_trunc'] = pd.Series(tv_austin_diff.index, index=tv_austin_diff.index).dt.floor('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model, predict, get probas\n",
    "X = tv_austin_diff.loc[:, good_feats]\n",
    "\n",
    "preds = lr.predict(X)\n",
    "probas = lr.predict_proba(X)\n",
    "probas = [element[1] for element in probas]\n",
    "\n",
    "most_recent_proba = probas[-1]\n",
    "\n",
    "# Prediction for time\n",
    "most_recent_time = tv_austin_diff.index.max()\n",
    "most_recent_time = most_recent_time.replace(tzinfo=pytz.utc)\n",
    "most_recent_time = most_recent_time.astimezone(pytz.timezone('America/Chicago'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rain_probability = '{:.1%}'.format(most_recent_proba)\n",
    "rain_qualifier = interpret_rain(most_recent_proba)\n",
    "\n",
    "html_statement = '<!DOCTYPE html><html><head><style type=\"text/css\">.karen_message{}</style><style type=\"text/css\">.qualifier_message{} </style></head><body><center><p class=\"karen_message\">This is Karen_Bot with the weather! <br><br>I am here in Austin and there is a <br><strong>{}</strong> chance that it is already raining! <br><br><i class=\"qualifier_message\">{}</i> </p></center> </body></html>'.format(\"{font-size: 16px};\",\"{font-size: 12px};\", _rain_probability, rain_qualifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"isitraining.html\",\"w+\")\n",
    "f.write(html_statement)\n",
    "f.close() \n",
    "\n",
    "f= open(\"update_date.txt\",\"w+\")\n",
    "f.write('{:%B %d, %Y at %H:%M} CST'.format(most_recent_time,most_recent_time ))\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy in new\n",
    "os.system(\"gsutil cp -r {}/isitraining.html {}\".format(cwd, f_path_full))\n",
    "os.system(\"gsutil cp -r {}/update_date.txt {}\".format(cwd, f_path_date))\n",
    "\n",
    "# set access public\n",
    "os.system(\"gsutil acl ch -u AllUsers:R {}\".format(f_path_full))\n",
    "os.system(\"gsutil acl ch -u AllUsers:R {}\".format(f_path_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

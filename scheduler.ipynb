{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "import twitter, re, datetime, pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import tweepy\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = pickle.load(open('vectorizer.sav', 'rb')) \n",
    "lr = pickle.load(open('lin_regressor.sav', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One time\n",
    "with open('./twitterapi.txt') as f:\n",
    "    ck, cs, atk, ats = f.read().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your keys go here:\n",
    "twitter_keys = {\n",
    "    'consumer_key':        ck,\n",
    "    'consumer_secret':     cs,\n",
    "    'access_token_key':    atk,\n",
    "    'access_token_secret': ats\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(twitter_keys['consumer_key'], twitter_keys['consumer_secret'])\n",
    "auth.set_access_token(twitter_keys['access_token_key'], twitter_keys['access_token_secret'])\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_converter(_time_input):\n",
    "    _time_zone_adjustment = 6\n",
    "    if _time_input < _time_zone_adjustment:\n",
    "        _hour_statement = str(_time_input - _time_zone_adjustment + 24) + \":00 PM\"\n",
    "    elif _time_input == _time_zone_adjustment:\n",
    "        _hour_statement = \"12:00 AM\"\n",
    "    elif _time_input < 19: \n",
    "        _hour_statement = str(_time_input - _time_zone_adjustment) + \":00 AM\"\n",
    "    else:\n",
    "        _hour_statement = str(_time_input - _time_zone_adjustment) + \":00 PM\"\n",
    "    return _hour_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_interval_hours = 6\n",
    "# for i in range(24):\n",
    "#     sleep(update_interval_hours*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomorrow = datetime.datetime.now() + datetime.timedelta(1)\n",
    "today = datetime.datetime.now()\n",
    "yesterday = datetime.datetime.now() - datetime.timedelta(1)\n",
    "end_date = '{}-{}-{}'.format(tomorrow.year, tomorrow.month, tomorrow.day)   \n",
    "start_date = '{}-{}-{}'.format(yesterday.year, yesterday.month, yesterday.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise, won't update public doc for 1 hour\n",
    "# !gsutil -h \"Cache-Control:no-cache, max-age=0\" cp -a public-read myfile.json gs://mybucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = pd.DataFrame()\n",
    "places = [\"Austin, TX\"]\n",
    "tweetsPerQry = 100\n",
    "counter = 0\n",
    "\n",
    "for place_q in places: \n",
    "\n",
    "    place = api.geo_search(query=place_q, granularity=\"city\")\n",
    "    place_id = place[0].id\n",
    "\n",
    "    max_id = -1\n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                new_tweets = api.search(q='place:%s' % place_id, count=tweetsPerQry, since=start_date, until=end_date)\n",
    "            else:\n",
    "                new_tweets = api.search(q='place:%s' % place_id, count=tweetsPerQry, max_id=str(max_id - 1), since=start_date, until=end_date)\n",
    "\n",
    "            df_text = pd.DataFrame([new_tweets[i]._json['text'] for i in range(len(new_tweets))], columns=['tweet_text'])\n",
    "            df_text['tweet_time']=[new_tweets[i]._json['created_at'] for i in range(len(new_tweets))]\n",
    "            df_text['tweet_place'] = place_q\n",
    "\n",
    "            df_small = pd.concat([df_small, df_text])\n",
    "            if not new_tweets:\n",
    "                counter += 1\n",
    "                df_small.to_csv('./data/collected_tweets_{}.csv'.format(counter)) # Should provide log of last good pull at least\n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError as e:\n",
    "            print('    all_done')\n",
    "            break\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use only the most recent tweets\n",
    "# If there are not enough, use a wider range\n",
    "df_small[\"tweet_time\"] = pd.to_datetime(df_small.tweet_time)\n",
    "if df_small[df_small.tweet_time > (datetime.datetime.now() - datetime.timedelta(hours=2))].shape[0] > 50:\n",
    "    df_small = df_small[df_small.tweet_time > (datetime.datetime.now() - datetime.timedelta(hours=2))]\n",
    "elif df_small == df_small[df_small.tweet_time > (datetime.datetime.now() - datetime.timedelta(hours=4))].shape[0]>50:\n",
    "    df_small = df_small[df_small.tweet_time > (datetime.datetime.now() - datetime.timedelta(hours=4))]\n",
    "else:\n",
    "    df_small = df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = cvec.get_feature_names()\n",
    "df_small = pd.concat([df_small.reset_index().drop('index', axis=1), pd.DataFrame(cvec.transform(df_small.tweet_text.str.lower()).todense(), columns=cvec.get_feature_names())], axis=1)\n",
    "_X = df_small.loc[:, [col for col in df_small.columns if col in model_columns]]\n",
    "\n",
    "_X.fillna(0, inplace=True)\n",
    "\n",
    "df_small['predicted'] = lr.predict(_X)\n",
    "df_small['probas'] = [element[1] for element in lr.predict_proba(_X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "excludes = ['tweet_text', 'tweet_time', 'tweet_place', 'is_rain', 'predicted']\n",
    "_rain_probability = df_small.loc[:, excludes + ['predicted', 'probas']].groupby('tweet_place').mean()['probas'].values[0]\n",
    "_rain_probability = round(_rain_probability*100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rain_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write qualifying statement separately for styling\n",
    "if _rain_probability < 20:\n",
    "    rain_qualifier = \"(so probably not)\"\n",
    "elif _rain_probability < 25:\n",
    "    rain_qualifier = \"(so maybe?)\"\n",
    "elif _rain_probability < 80:\n",
    "    rain_qualifier = \"(so probably!)\"\n",
    "else:\n",
    "    rain_qualifier = \"Its definitely raining!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_statement = '<!DOCTYPE html><html><head><style type=\"text/css\">.karen_message{}</style><style type=\"text/css\">.qualifier_message{} </style></head><body><center><p class=\"karen_message\">This is Karen_Bot with the weather! <br><br>I am here in Austin and there is a <br><strong>{}%</strong> chance that it is already raining! <br><br><i class=\"qualifier_message\">{}</i> </p></center> </body></html>'.format(\"{font-size: 16px};\",\"{font-size: 12px};\", _rain_probability, rain_qualifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><html><head><style type=\"text/css\">.karen_message{font-size: 16px};</style><style type=\"text/css\">.qualifier_message{font-size: 12px}; </style></head><body><center><p class=\"karen_message\">This is Karen_Bot with the weather! <br><br>I am here in Austin and there is a <br><strong>59.0%</strong> chance that it is already raining! <br><br><i class=\"qualifier_message\">(so probably!)</i> </p></center> </body></html>'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write text doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"isitraining.html\",\"w+\")\n",
    "f.write(html_statement)\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'January 23, 2019 at 18:00 PM'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{} {}, {} at {}'.format(today.strftime(\"%B\"), _day, _year, hour_converter(_hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"update_date.txt\",\"w+\")\n",
    "f.write('{} {}, {} at {}'.format(datetime.datetime.now().strftime(\"%B\"), _day, _year, hour_converter(_hour)))\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #set file private\n",
    "# os.system(\"gsutil acl set private gs://is-it-raining/isitraining.txt\")\n",
    "\n",
    "# # Delete existing\n",
    "# os.system(\"gsutil rm gs://is-it-raining/isitraining.txt\")\n",
    "\n",
    "# copy in new\n",
    "os.system(\"gsutil cp -r ~/isitraining.html gs://is-it-raining/\")\n",
    "os.system(\"gsutil cp -r ~/update_date.txt gs://is-it-raining/\")\n",
    "\n",
    "# set access public\n",
    "os.system(\"gsutil acl ch -u AllUsers:R gs://is-it-raining/isitraining.html\")\n",
    "os.system(\"gsutil acl ch -u AllUsers:R gs://is-it-raining/update_date.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the log to record date, time, and prediction\n",
    "update_log = pd.read_csv('./update_log.csv')\n",
    "update_log = pd.concat([update_log, pd.DataFrame({'Date': datetime.datetime.now(), 'rain_statement': _rain_probability}, index=[0])])\n",
    "update_log.to_csv('./update_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

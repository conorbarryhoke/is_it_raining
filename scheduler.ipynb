{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "import twitter, re, datetime, pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import tweepy\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = pickle.load(open('vectorizer.sav', 'rb')) \n",
    "lr = pickle.load(open('lin_regressor.sav', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One time\n",
    "with open('./twitterapi.txt') as f:\n",
    "    ck, cs, atk, ats = f.read().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your keys go here:\n",
    "twitter_keys = {\n",
    "    'consumer_key':        ck,\n",
    "    'consumer_secret':     cs,\n",
    "    'access_token_key':    atk,\n",
    "    'access_token_secret': ats\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(twitter_keys['consumer_key'], twitter_keys['consumer_secret'])\n",
    "auth.set_access_token(twitter_keys['access_token_key'], twitter_keys['access_token_secret'])\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_converter(_time_input):\n",
    "    _time_zone_adjustment = 6\n",
    "    if _time_input < _time_zone_adjustment:\n",
    "        _hour_statement = str(_time_input - _time_zone_adjustment + 24) + \":00 PM\"\n",
    "    elif _time_input == _time_zone_adjustment:\n",
    "        _hour_statement = \"12:00 AM\"\n",
    "    elif _time_input < 19: \n",
    "        _hour_statement = str(_time_input - _time_zone_adjustment) + \":00 AM\"\n",
    "    else:\n",
    "        _hour_statement = str(_time_input - _time_zone_adjustment) + \":00 PM\"\n",
    "    return _hour_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_interval_hours = 6\n",
    "# for i in range(24):\n",
    "#     sleep(update_interval_hours*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_year = datetime.datetime.now().year\n",
    "_month = datetime.datetime.now().month\n",
    "_day = datetime.datetime.now().day\n",
    "_hour = datetime.datetime.now().hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each Time\n",
    "\n",
    "end_date = '{}-{}-{}'.format(_year, _month, _day + 1, _hour)\n",
    "if _month == 1 & _day ==1:\n",
    "    _year == _year-1\n",
    "    _month == 12\n",
    "    _day == 31\n",
    "elif _day ==1:\n",
    "    _year = _year\n",
    "    _month = _month -1\n",
    "    _day == 28\n",
    "else:\n",
    "    _year = _year\n",
    "    _month = _month\n",
    "    _day = _day - 1\n",
    "    \n",
    "start_date = '{}-{}-{}'.format(_year, _month, _day, _hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise, won't update public doc for 1 hour\n",
    "# !gsutil -h \"Cache-Control:no-cache, max-age=0\" cp -a public-read myfile.json gs://mybucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_big = pd.DataFrame()\n",
    "df_small = pd.DataFrame()\n",
    "# places = [\"Shreveport, LA\",'San Francisco, CA', \"Austin, TX\", \"Portland, OR\", \"Boston, MA\"]\n",
    "places = [\"Austin, TX\"]\n",
    "tweetsPerQry = 100\n",
    "counter = 0\n",
    "\n",
    "for place_q in places: \n",
    "\n",
    "    place = api.geo_search(query=place_q, granularity=\"city\")\n",
    "    place_id = place[0].id\n",
    "\n",
    "    max_id = -1\n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                new_tweets = api.search(q='place:%s' % place_id, count=tweetsPerQry, since=start_date, until=end_date)\n",
    "            else:\n",
    "                new_tweets = api.search(q='place:%s' % place_id, count=tweetsPerQry, max_id=str(max_id - 1), since=start_date, until=end_date)\n",
    "\n",
    "            df_text = pd.DataFrame([new_tweets[i]._json['text'] for i in range(len(new_tweets))], columns=['tweet_text'])\n",
    "            df_text['tweet_time']=[new_tweets[i]._json['created_at'] for i in range(len(new_tweets))]\n",
    "            df_text['tweet_place'] = place_q\n",
    "\n",
    "            df_small = pd.concat([df_small, df_text])\n",
    "#             df_big = pd.concat([df_big, df_text])\n",
    "            if not new_tweets:\n",
    "                counter += 1\n",
    "                df_small.to_csv('./data/collected_tweets_{}.csv'.format(counter)) # Should provide log of last good pull at least\n",
    "#                 df_big.to_csv('./data/collected_tweets_big_{}.csv'.format(counter)) \n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError as e:\n",
    "            print('    all_done')\n",
    "            break\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = cvec.get_feature_names()\n",
    "df_small = pd.concat([df_small.reset_index().drop('index', axis=1), pd.DataFrame(cvec.transform(df_small.tweet_text.str.lower()).todense(), columns=cvec.get_feature_names())], axis=1)\n",
    "_X = df_small.loc[:, [col for col in df_small.columns if col in model_columns]]\n",
    "\n",
    "_X.fillna(0, inplace=True)\n",
    "\n",
    "df_small['predicted'] = lr.predict(_X)\n",
    "df_small['probas'] = [element[1] for element in lr.predict_proba(_X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "excludes = ['tweet_text', 'tweet_time', 'tweet_place', 'is_rain', 'predicted']\n",
    "_rain_probability = df_small.loc[:, excludes + ['predicted', 'probas']].groupby('tweet_place').mean()['probas'].values[0]\n",
    "_rain_probability = round(_rain_probability*100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write text doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"isitraining.txt\",\"w+\")\n",
    "f.write(str(_rain_probability))\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'January 17, 2019 at 11:00 AM'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{} {}, {} at {}'.format(datetime.datetime.now().strftime(\"%B\"), _day, _year, hour_converter(_hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"update_date.txt\",\"w+\")\n",
    "f.write('{} {}, {} at {}'.format(datetime.datetime.now().strftime(\"%B\"), _day, _year, hour_converter(_hour)))\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #set file private\n",
    "# os.system(\"gsutil acl set private gs://is-it-raining/isitraining.txt\")\n",
    "\n",
    "# # Delete existing\n",
    "# os.system(\"gsutil rm gs://is-it-raining/isitraining.txt\")\n",
    "\n",
    "# copy in new\n",
    "os.system(\"gsutil cp -r ~/isitraining.txt gs://is-it-raining/\")\n",
    "os.system(\"gsutil cp -r ~/update_date.txt gs://is-it-raining/\")\n",
    "\n",
    "# set access public\n",
    "os.system(\"gsutil acl ch -u AllUsers:R gs://is-it-raining/isitraining.txt\")\n",
    "os.system(\"gsutil acl ch -u AllUsers:R gs://is-it-raining/update_date.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the log to record date, time, and prediction\n",
    "update_log = pd.read_csv('./update_log.csv')\n",
    "update_log = pd.concat([update_log, pd.DataFrame({'Date': datetime.datetime.now(), 'rain_statement': _rain_probability}, index=[0])])\n",
    "update_log.to_csv('./update_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

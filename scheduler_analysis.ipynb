{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "import twitter, re, datetime, pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import tweepy\n",
    "from pprint import pprint \n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big=pd.concat([\n",
    "    pd.read_csv('./data/5_cities_2019.1.3.csv'), \n",
    "    pd.read_csv('./data/5_cities_2019.1.17.csv')\n",
    "])\n",
    "df_big.index = range(df_big.shape[0])\n",
    "df_big[\"tweet_time\"] = pd.to_datetime(df_big.tweet_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_place</th>\n",
       "      <th>tweet_time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Austin, TX</th>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-17</th>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Boston, MA</th>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-17</th>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Portland, OR</th>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-17</th>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">San Francisco, CA</th>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-17</th>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Shreveport, LA</th>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-17</th>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              tweet_text\n",
       "tweet_place       tweet_time            \n",
       "Austin, TX        2019-01-03         358\n",
       "                  2019-01-17         788\n",
       "Boston, MA        2019-01-03          82\n",
       "                  2019-01-17         625\n",
       "Portland, OR      2019-01-03         442\n",
       "                  2019-01-17         654\n",
       "San Francisco, CA 2019-01-03         316\n",
       "                  2019-01-17         654\n",
       "Shreveport, LA    2019-01-03         455\n",
       "                  2019-01-17         474"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_big.groupby(['tweet_place', df_big.tweet_time.dt.date]).count().loc[:, [\"tweet_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "\n",
    "df_big=df_big.join(pd.DataFrame(cvec.fit_transform(df_big.tweet_text.str.lower()).todense(), columns=cvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big[\"tweet_time\"] = pd.to_datetime(df_big.tweet_time)\n",
    "df_big['is_rain']= (((df_big.tweet_place == \"Shreveport, LA\") & (df_big.tweet_time.dt.date == datetime.date(2019, 1, 3))) |\\\n",
    "                    ((df_big.tweet_place == \"San Francisco, CA\") & (df_big.tweet_time.dt.date == datetime.date(2019, 1, 17))))*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "excludes = ['tweet_text', 'tweet_time', 'tweet_place', 'is_rain', 'predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_big.loc[:, [col for col in df_big.columns if col not in excludes]]\n",
    "y = df_big.is_rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7805280528052805\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_rain</th>\n",
       "      <th>predicted</th>\n",
       "      <th>predicted</th>\n",
       "      <th>probas</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_place</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austin, TX</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011344</td>\n",
       "      <td>0.011344</td>\n",
       "      <td>0.131639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Boston, MA</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008487</td>\n",
       "      <td>0.008487</td>\n",
       "      <td>0.134202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portland, OR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006387</td>\n",
       "      <td>0.006387</td>\n",
       "      <td>0.116531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>San Francisco, CA</th>\n",
       "      <td>0.674227</td>\n",
       "      <td>0.451546</td>\n",
       "      <td>0.451546</td>\n",
       "      <td>0.428521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shreveport, LA</th>\n",
       "      <td>0.489774</td>\n",
       "      <td>0.274489</td>\n",
       "      <td>0.274489</td>\n",
       "      <td>0.327072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    is_rain  predicted  predicted    probas\n",
       "tweet_place                                                \n",
       "Austin, TX         0.000000   0.011344   0.011344  0.131639\n",
       "Boston, MA         0.000000   0.008487   0.008487  0.134202\n",
       "Portland, OR       0.000000   0.006387   0.006387  0.116531\n",
       "San Francisco, CA  0.674227   0.451546   0.451546  0.428521\n",
       "Shreveport, LA     0.489774   0.274489   0.274489  0.327072"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_test, y_test))\n",
    "\n",
    "df_big['predicted'] = lr.predict(X)\n",
    "df_big['probas'] = [element[1] for element in lr.predict_proba(X)]\n",
    "\n",
    "df_big.loc[:, excludes + ['predicted', 'probas']].groupby('tweet_place').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12625038624702614"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_big[(df_big.tweet_place==\"Austin, TX\")&(df_big.tweet_time.dt.date>datetime.date(2019,1,4))].loc[:, 'probas'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "probas    0.13503\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doesn't seem to help too much filter \n",
    "df_big[(df_big.tweet_place==\"Austin, TX\")&(df_big.tweet_time.dt.date>=datetime.date(2019,1,17))]\\\n",
    ".loc[:, ['tweet_time','probas']].sort_values(by='tweet_time').tail(20).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(cvec, open('vectorizer.sav', 'wb')) \n",
    "# pickle.dump(lr, open('lin_regressor.sav', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = pickle.load(open('vectorizer.sav', 'rb')) \n",
    "lr = pickle.load(open('lin_regressor.sav', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check small frame\n",
    "df_small = pd.read_csv('./data/austin_2019-01-27_to_2019-01-28.csv')\n",
    "df_small[\"tweet_time\"] = pd.to_datetime(df_small.tweet_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 to 2 hours ago had 363 tweets\n",
      "2 to 3 hours ago had 291 tweets\n",
      "3 to 4 hours ago had 190 tweets\n",
      "4 to 5 hours ago had 120 tweets\n",
      "5 to 6 hours ago had 57 tweets\n",
      "6 to 7 hours ago had 53 tweets\n",
      "7 to 8 hours ago had 77 tweets\n",
      "8 to 9 hours ago had 81 tweets\n",
      "9 to 10 hours ago had 67 tweets\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    print('{} to {} hours ago had {} tweets'.format(i , i+1, \n",
    "                                                   df_small[(df_small.tweet_time >= (datetime.datetime.now() - datetime.timedelta(hours=i+1)))&\\\n",
    "                                                          ( df_small.tweet_time <= (datetime.datetime.now() - datetime.timedelta(hours=i)))].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_rain</th>\n",
       "      <th>predicted</th>\n",
       "      <th>predicted</th>\n",
       "      <th>probas</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_place</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austin, TX</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033199</td>\n",
       "      <td>0.033199</td>\n",
       "      <td>0.195745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             is_rain  predicted  predicted    probas\n",
       "tweet_place                                         \n",
       "Austin, TX       NaN   0.033199   0.033199  0.195745"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small = df_small[df_small.tweet_time > (datetime.datetime.now() - datetime.timedelta(hours=4))] # Expand here to 6 if shape[0], then allow all\n",
    "df_small = pd.concat([df_small, pd.DataFrame(cvec.transform(df_small.tweet_text.str.lower()).todense(), columns=cvec.get_feature_names())], axis=1)\n",
    "_X = df_small.loc[:, [col for col in df_small.columns if col in X.columns]]\n",
    "\n",
    "_X.fillna(0, inplace=True)\n",
    "\n",
    "df_small['predicted'] = lr.predict(_X)\n",
    "df_small['probas'] = [element[1] for element in lr.predict_proba(_X)]\n",
    "\n",
    "df_small.loc[:, excludes + ['predicted', 'probas']].groupby('tweet_place').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = pickle.load(open('vectorizer.sav', 'rb')) \n",
    "lr = pickle.load(open('lin_regressor.sav', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One time\n",
    "with open('./twitterapi.txt') as f:\n",
    "    ck, cs, atk, ats = f.read().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your keys go here:\n",
    "twitter_keys = {\n",
    "    'consumer_key':        ck,\n",
    "    'consumer_secret':     cs,\n",
    "    'access_token_key':    atk,\n",
    "    'access_token_secret': ats\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(twitter_keys['consumer_key'], twitter_keys['consumer_secret'])\n",
    "auth.set_access_token(twitter_keys['access_token_key'], twitter_keys['access_token_secret'])\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_interval_hours = 6\n",
    "# for i in range(24):\n",
    "#     sleep(update_interval_hours*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomorrow = datetime.datetime.now() + datetime.timedelta(1)\n",
    "yesterday = datetime.datetime.now() - datetime.timedelta(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each Time\n",
    "end_date = '{}-{}-{}'.format(tomorrow.year, tomorrow.month, tomorrow.day)   \n",
    "start_date = '{}-{}-{}'.format(yesterday.year, yesterday.month, yesterday.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise, won't update public doc for 1 hour\n",
    "# !gsutil -h \"Cache-Control:no-cache, max-age=0\" cp -a public-read myfile.json gs://mybucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_big = False\n",
    "if create_big:\n",
    "    df_big = pd.DataFrame()\n",
    "    places = [\"Shreveport, LA\",'San Francisco, CA', \"Austin, TX\", \"Portland, OR\", \"Boston, MA\"]\n",
    "else:\n",
    "    df_small = pd.DataFrame()\n",
    "    places = [\"Austin, TX\"]\n",
    "tweetsPerQry = 100\n",
    "counter = 0\n",
    "\n",
    "for place_q in places: \n",
    "\n",
    "    place = api.geo_search(query=place_q, granularity=\"city\")\n",
    "    place_id = place[0].id\n",
    "\n",
    "    max_id = -1\n",
    "    for _ in range(20):\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                new_tweets = api.search(q='place:%s' % place_id, count=tweetsPerQry, since=start_date, until=end_date)\n",
    "            else:\n",
    "                new_tweets = api.search(q='place:%s' % place_id, count=tweetsPerQry, max_id=str(max_id - 1), since=start_date, until=end_date)\n",
    "\n",
    "            df_text = pd.DataFrame([new_tweets[i]._json['text'] for i in range(len(new_tweets))], columns=['tweet_text'])\n",
    "            df_text['tweet_time']=[new_tweets[i]._json['created_at'] for i in range(len(new_tweets))]\n",
    "            df_text['tweet_place'] = place_q\n",
    "            if create_big:\n",
    "                df_big = pd.concat([df_big, df_text])\n",
    "            else:\n",
    "                df_small = pd.concat([df_small, df_text])\n",
    "            \n",
    "            if not new_tweets:\n",
    "                counter += 1\n",
    "                if create_big:\n",
    "                    df_big.to_csv('./collected_tweets_{}.csv'.format(counter)) \n",
    "                else: \n",
    "                    df_small.to_csv('./collected_tweets_{}.csv'.format(counter)) # Should provide log of last good pull at least\n",
    "                    \n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError as e:\n",
    "            print('    all_done')\n",
    "            break\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small.to_csv('./data/austin_2019-01-27_to_2019-01-28.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_big.to_csv('./5_cities_2019.1.17.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = cvec.get_feature_names()\n",
    "df_small = pd.concat([df_small.reset_index().drop('index', axis=1), pd.DataFrame(cvec.transform(df_small.tweet_text.str.lower()).todense(), columns=cvec.get_feature_names())], axis=1)\n",
    "_X = df_small.loc[:, [col for col in df_small.columns if col in model_columns]]\n",
    "\n",
    "_X.fillna(0, inplace=True)\n",
    "\n",
    "df_small['predicted'] = lr.predict(_X)\n",
    "df_small['probas'] = [element[1] for element in lr.predict_proba(_X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "excludes = ['tweet_text', 'tweet_time', 'tweet_place', 'is_rain', 'predicted']\n",
    "_rain_probability = df_small.loc[:, excludes + ['predicted', 'probas']].groupby('tweet_place').mean()['probas'].values[0]\n",
    "_rain_probability = round(_rain_probability*100, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

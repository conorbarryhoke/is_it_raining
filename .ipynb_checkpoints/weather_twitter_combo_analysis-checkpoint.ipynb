{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "import twitter, re, datetime, pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import tweepy\n",
    "from pprint import pprint \n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get historic weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_path = \"http://storage.googleapis.com/twitter-weather/weather-data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./info/weather_key.txt', \"r\")\n",
    "f.read()\n",
    "api_key = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/python-find-current-weather-of-any-city-using-openweathermap-api/\n",
    "# Enter your API key here \n",
    "api_key = \"Your_API_Key\"\n",
    "  \n",
    "# base_url variable to store url \n",
    "base_url = \"http://api.openweathermap.org/data/2.5/weather?\"\n",
    "  \n",
    "# Give city name \n",
    "city_name = input(\"Enter city name : \") \n",
    "  \n",
    "# complete_url variable to store \n",
    "# complete url address \n",
    "complete_url = base_url + \"appid=\" + api_key + \"&q=\" + city_name \n",
    "  \n",
    "# get method of requests module \n",
    "# return response object \n",
    "response = requests.get(complete_url) \n",
    "  \n",
    "# json method of response object  \n",
    "# convert json format data into \n",
    "# python format data \n",
    "x = response.json() \n",
    "  \n",
    "# Now x contains list of nested dictionaries \n",
    "# Check the value of \"cod\" key is equal to \n",
    "# \"404\", means city is found otherwise, \n",
    "# city is not found \n",
    "if x[\"cod\"] != \"404\": \n",
    "  \n",
    "    # store the value of \"main\" \n",
    "    # key in variable y \n",
    "    y = x[\"main\"] \n",
    "  \n",
    "    # store the value corresponding \n",
    "    # to the \"temp\" key of y \n",
    "    current_temperature = y[\"temp\"] \n",
    "  \n",
    "    # store the value corresponding \n",
    "    # to the \"pressure\" key of y \n",
    "    current_pressure = y[\"pressure\"] \n",
    "  \n",
    "    # store the value corresponding \n",
    "    # to the \"humidity\" key of y \n",
    "    current_humidiy = y[\"humidity\"] \n",
    "  \n",
    "    # store the value of \"weather\" \n",
    "    # key in variable z \n",
    "    z = x[\"weather\"] \n",
    "  \n",
    "    # store the value corresponding  \n",
    "    # to the \"description\" key at  \n",
    "    # the 0th index of z \n",
    "    weather_description = z[0][\"description\"] \n",
    "  \n",
    "    # print following values \n",
    "    print(\" Temperature (in kelvin unit) = \" +\n",
    "                    str(current_temperature) + \n",
    "          \"\\n atmospheric pressure (in hPa unit) = \" +\n",
    "                    str(current_pressure) +\n",
    "          \"\\n humidity (in percentage) = \" +\n",
    "                    str(current_humidiy) +\n",
    "          \"\\n description = \" +\n",
    "                    str(weather_description)) \n",
    "  \n",
    "else: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big=pd.concat([\n",
    "    pd.read_csv('./data/5_cities_2019.1.3.csv'), \n",
    "    pd.read_csv('./data/5_cities_2019.1.17.csv'), \n",
    "#     pd.read_csv('./data/austin_2019_02_19.csv')\n",
    "])\n",
    "df_big.index = range(df_big.shape[0])\n",
    "df_big[\"tweet_time\"] = pd.to_datetime(df_big.tweet_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_place</th>\n",
       "      <th>tweet_time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">Austin, TX</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">Boston, MA</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">Portland, OR</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">San Francisco, CA</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">Shreveport, LA</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              tweet_text\n",
       "tweet_place       tweet_time            \n",
       "Austin, TX        2019-01-03         358\n",
       "                  2019-01-17         788\n",
       "Boston, MA        2019-01-03          82\n",
       "                  2019-01-17         625\n",
       "Portland, OR      2019-01-03         442\n",
       "                  2019-01-17         654\n",
       "San Francisco, CA 2019-01-03         316\n",
       "                  2019-01-17         654\n",
       "Shreveport, LA    2019-01-03         455\n",
       "                  2019-01-17         474"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_big.groupby(['tweet_place', df_big.tweet_time.dt.date]).count().loc[:, [\"tweet_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "\n",
    "df_big=df_big.join(pd.DataFrame(cvec.fit_transform(df_big.tweet_text.str.lower()).todense(), columns=cvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big[\"tweet_time\"] = pd.to_datetime(df_big.tweet_time)\n",
    "df_big['is_rain']= (((df_big.tweet_place == \"Shreveport, LA\") & (df_big.tweet_time.dt.date == datetime.date(2019, 1, 3))) |\\\n",
    "                    ((df_big.tweet_place == \"San Francisco, CA\") & (df_big.tweet_time.dt.date == datetime.date(2019, 1, 17)))|\n",
    "                    ((df_big.tweet_place == \"Austin, TX\") & (df_big.tweet_time.dt.date == datetime.date(2019, 2, 20)))\n",
    "                   )*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "excludes = ['tweet_text', 'tweet_time', 'tweet_place', 'is_rain', 'predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_big.loc[:, [col for col in df_big.columns if col not in excludes]]\n",
    "y = df_big.is_rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6125721616420783\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_rain</th>\n",
       "      <th>predicted</th>\n",
       "      <th>predicted</th>\n",
       "      <th>probas</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_place</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austin, TX</th>\n",
       "      <td>0.547393</td>\n",
       "      <td>0.467615</td>\n",
       "      <td>0.467615</td>\n",
       "      <td>0.463345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Boston, MA</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052334</td>\n",
       "      <td>0.052334</td>\n",
       "      <td>0.224023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portland, OR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043796</td>\n",
       "      <td>0.043796</td>\n",
       "      <td>0.200735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>San Francisco, CA</th>\n",
       "      <td>0.674227</td>\n",
       "      <td>0.555670</td>\n",
       "      <td>0.555670</td>\n",
       "      <td>0.516902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shreveport, LA</th>\n",
       "      <td>0.489774</td>\n",
       "      <td>0.392896</td>\n",
       "      <td>0.392896</td>\n",
       "      <td>0.432043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    is_rain  predicted  predicted    probas\n",
       "tweet_place                                                \n",
       "Austin, TX         0.547393   0.467615   0.467615  0.463345\n",
       "Boston, MA         0.000000   0.052334   0.052334  0.224023\n",
       "Portland, OR       0.000000   0.043796   0.043796  0.200735\n",
       "San Francisco, CA  0.674227   0.555670   0.555670  0.516902\n",
       "Shreveport, LA     0.489774   0.392896   0.392896  0.432043"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_test, y_test))\n",
    "\n",
    "df_big['predicted'] = lr.predict(X)\n",
    "df_big['probas'] = [element[1] for element in lr.predict_proba(X)]\n",
    "\n",
    "df_big.loc[:, excludes + ['predicted', 'probas']].groupby('tweet_place').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.648584596895982"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_big[(df_big.tweet_place==\"Austin, TX\")&(df_big.tweet_time.dt.date>datetime.date(2019,2,1))].loc[:, 'probas'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "probas    0.646408\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doesn't seem to help too much filter \n",
    "df_big[(df_big.tweet_place==\"Austin, TX\")&(df_big.tweet_time.dt.date>=datetime.date(2019,2,17))]\\\n",
    ".loc[:, ['tweet_time','probas']].sort_values(by='tweet_time').tail(20).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(cvec, open('vectorizer.sav', 'wb')) \n",
    "# pickle.dump(lr, open('lin_regressor.sav', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = pickle.load(open('vectorizer.sav', 'rb')) \n",
    "lr = pickle.load(open('lin_regressor.sav', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check small frame\n",
    "df_small = pd.read_csv('./data/austin_2019-01-27_to_2019-01-28.csv')\n",
    "df_small[\"tweet_time\"] = pd.to_datetime(df_small.tweet_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 to 2 hours ago had 363 tweets\n",
      "2 to 3 hours ago had 291 tweets\n",
      "3 to 4 hours ago had 190 tweets\n",
      "4 to 5 hours ago had 120 tweets\n",
      "5 to 6 hours ago had 57 tweets\n",
      "6 to 7 hours ago had 53 tweets\n",
      "7 to 8 hours ago had 77 tweets\n",
      "8 to 9 hours ago had 81 tweets\n",
      "9 to 10 hours ago had 67 tweets\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    print('{} to {} hours ago had {} tweets'.format(i , i+1, \n",
    "                                                   df_small[(df_small.tweet_time >= (datetime.datetime.now() - datetime.timedelta(hours=i+1)))&\\\n",
    "                                                          ( df_small.tweet_time <= (datetime.datetime.now() - datetime.timedelta(hours=i)))].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_small = df_small[df_small.tweet_time > (datetime.datetime.now() - datetime.timedelta(hours=4))] # Expand here to 6 if shape[0], then allow all\n",
    "df_small = pd.concat([df_small, pd.DataFrame(cvec.transform(df_small.tweet_text.str.lower()).todense(), columns=cvec.get_feature_names())], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = df_small.loc[:, [col for col in df_small.columns if col in X.columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_rain</th>\n",
       "      <th>predicted</th>\n",
       "      <th>predicted</th>\n",
       "      <th>probas</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_place</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austin, TX</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254275</td>\n",
       "      <td>0.254275</td>\n",
       "      <td>0.395661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             is_rain  predicted  predicted    probas\n",
       "tweet_place                                         \n",
       "Austin, TX       NaN   0.254275   0.254275  0.395661"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_X.fillna(0, inplace=True)\n",
    "\n",
    "df_small['predicted'] = lr.predict(_X)\n",
    "df_small['probas'] = [element[1] for element in lr.predict_proba(_X)]\n",
    "\n",
    "df_small.loc[:, excludes + ['predicted', 'probas']].groupby('tweet_place').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get New Data (Script Needs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = pickle.load(open('vectorizer.sav', 'rb')) \n",
    "lr = pickle.load(open('lin_regressor.sav', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One time\n",
    "with open('./twitterapi.txt') as f:\n",
    "    ck, cs, atk, ats = f.read().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your keys go here:\n",
    "twitter_keys = {\n",
    "    'consumer_key':        ck,\n",
    "    'consumer_secret':     cs,\n",
    "    'access_token_key':    atk,\n",
    "    'access_token_secret': ats\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(twitter_keys['consumer_key'], twitter_keys['consumer_secret'])\n",
    "auth.set_access_token(twitter_keys['access_token_key'], twitter_keys['access_token_secret'])\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_interval_hours = 6\n",
    "# for i in range(24):\n",
    "#     sleep(update_interval_hours*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomorrow = datetime.datetime.now() + datetime.timedelta(1)\n",
    "yesterday = datetime.datetime.now() - datetime.timedelta(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each Time\n",
    "end_date = '{}-{}-{}'.format(tomorrow.year, tomorrow.month, tomorrow.day)   \n",
    "start_date = '{}-{}-{}'.format(yesterday.year, yesterday.month, yesterday.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise, won't update public doc for 1 hour\n",
    "# !gsutil -h \"Cache-Control:no-cache, max-age=0\" cp -a public-read myfile.json gs://mybucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_big = False\n",
    "if create_big:\n",
    "    df_big = pd.DataFrame()\n",
    "    places = [\"Shreveport, LA\",'San Francisco, CA', \"Austin, TX\", \"Portland, OR\", \"Boston, MA\"]\n",
    "else:\n",
    "    df_small = pd.DataFrame()\n",
    "#     places = [\"Austin, TX\"]\n",
    "    places = [\"Austin, TX\"]\n",
    "tweetsPerQry = 100\n",
    "counter = 0\n",
    "\n",
    "for place_q in places: \n",
    "\n",
    "    place = api.geo_search(query=place_q, granularity=\"city\")\n",
    "    place_id = place[0].id\n",
    "\n",
    "    max_id = -1\n",
    "    for _ in range(20):\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                new_tweets = api.search(q='place:%s' % place_id, count=tweetsPerQry, since=start_date, until=end_date)\n",
    "            else:\n",
    "                new_tweets = api.search(q='place:%s' % place_id, count=tweetsPerQry, max_id=str(max_id - 1), since=start_date, until=end_date)\n",
    "\n",
    "            df_text = pd.DataFrame([new_tweets[i]._json['text'] for i in range(len(new_tweets))], columns=['tweet_text'])\n",
    "            df_text['tweet_time']=[new_tweets[i]._json['created_at'] for i in range(len(new_tweets))]\n",
    "            df_text['tweet_place'] = place_q\n",
    "            if create_big:\n",
    "                df_big = pd.concat([df_big, df_text])\n",
    "            else:\n",
    "                df_small = pd.concat([df_small, df_text])\n",
    "            \n",
    "            if not new_tweets:\n",
    "                counter += 1\n",
    "                if create_big:\n",
    "                    df_big.to_csv('./collected_tweets_{}_{}.csv'.format(start_date, counter)) \n",
    "                else: \n",
    "                    df_small.to_csv('./collected_tweets_{}_{}.csv'.format(start_date, counter)) # Should provide log of last good pull at least\n",
    "                    \n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError as e:\n",
    "            print('    all_done')\n",
    "            break\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small.to_csv('./data/austin_2019_02_19.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this cell to transform chart to be added in to df_big\n",
    "df_small[\"tweet_time\"] = pd.to_datetime(df_small.tweet_time)\n",
    "df_small = df_small[df_small.tweet_time > (datetime.datetime.now() - datetime.timedelta(hours=6))] # For sure raining here\n",
    "df_small.to_csv('./data/santa_barbara_2019-01-30_to_2019-01-31.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_big.to_csv('./5_cities_2019.1.17.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = cvec.get_feature_names()\n",
    "df_small = pd.concat([df_small.reset_index().drop('index', axis=1), pd.DataFrame(cvec.transform(df_small.tweet_text.str.lower()).todense(), columns=cvec.get_feature_names())], axis=1)\n",
    "_X = df_small.loc[:, [col for col in df_small.columns if col in model_columns]]\n",
    "\n",
    "_X.fillna(0, inplace=True)\n",
    "\n",
    "df_small['predicted'] = lr.predict(_X)\n",
    "df_small['probas'] = [element[1] for element in lr.predict_proba(_X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "excludes = ['tweet_text', 'tweet_time', 'tweet_place', 'is_rain', 'predicted']\n",
    "_rain_probability = df_small.loc[:, excludes + ['predicted', 'probas']].groupby('tweet_place').mean()['probas'].values[0]\n",
    "_rain_probability = round(_rain_probability*100, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
